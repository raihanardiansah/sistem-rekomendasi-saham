{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2ec7d00",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q PySastrawi beautifulsoup4 lxml requests pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebcf78b",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6f7afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5f8725",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Daftar Saham Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed5ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daftar Saham IHSG Populer\n",
    "STOCKS_DATA = {\n",
    "    # Banking\n",
    "    'BBCA': {'nama': 'Bank Central Asia', 'sektor': 'Keuangan', 'index': 'LQ45,IDX30'},\n",
    "    'BBRI': {'nama': 'Bank Rakyat Indonesia', 'sektor': 'Keuangan', 'index': 'LQ45,IDX30'},\n",
    "    'BMRI': {'nama': 'Bank Mandiri', 'sektor': 'Keuangan', 'index': 'LQ45,IDX30'},\n",
    "    'BBNI': {'nama': 'Bank Negara Indonesia', 'sektor': 'Keuangan', 'index': 'LQ45,IDX30'},\n",
    "    'BRIS': {'nama': 'Bank Syariah Indonesia', 'sektor': 'Keuangan', 'index': 'LQ45'},\n",
    "    \n",
    "    # Telekomunikasi\n",
    "    'TLKM': {'nama': 'Telkom Indonesia', 'sektor': 'Telekomunikasi', 'index': 'LQ45,IDX30'},\n",
    "    'EXCL': {'nama': 'XL Axiata', 'sektor': 'Telekomunikasi', 'index': 'LQ45'},\n",
    "    'ISAT': {'nama': 'Indosat Ooredoo', 'sektor': 'Telekomunikasi', 'index': 'LQ45'},\n",
    "    \n",
    "    # Consumer\n",
    "    'UNVR': {'nama': 'Unilever Indonesia', 'sektor': 'Consumer Goods', 'index': 'LQ45,IDX30'},\n",
    "    'ICBP': {'nama': 'Indofood CBP', 'sektor': 'Consumer Goods', 'index': 'LQ45,IDX30'},\n",
    "    'INDF': {'nama': 'Indofood Sukses Makmur', 'sektor': 'Consumer Goods', 'index': 'LQ45'},\n",
    "    'MYOR': {'nama': 'Mayora Indah', 'sektor': 'Consumer Goods', 'index': 'LQ45'},\n",
    "    \n",
    "    # Mining & Energy\n",
    "    'ADRO': {'nama': 'Adaro Energy', 'sektor': 'Pertambangan', 'index': 'LQ45,IDX30'},\n",
    "    'PTBA': {'nama': 'Bukit Asam', 'sektor': 'Pertambangan', 'index': 'LQ45,IDX30'},\n",
    "    'ITMG': {'nama': 'Indo Tambangraya Megah', 'sektor': 'Pertambangan', 'index': 'LQ45'},\n",
    "    'ANTM': {'nama': 'Aneka Tambang', 'sektor': 'Pertambangan', 'index': 'LQ45,IDX30'},\n",
    "    'INCO': {'nama': 'Vale Indonesia', 'sektor': 'Pertambangan', 'index': 'LQ45'},\n",
    "    'MDKA': {'nama': 'Merdeka Copper Gold', 'sektor': 'Pertambangan', 'index': 'LQ45'},\n",
    "    \n",
    "    # Automotive\n",
    "    'ASII': {'nama': 'Astra International', 'sektor': 'Otomotif', 'index': 'LQ45,IDX30'},\n",
    "    'AUTO': {'nama': 'Astra Otoparts', 'sektor': 'Otomotif', 'index': 'LQ45'},\n",
    "    \n",
    "    # Property\n",
    "    'BSDE': {'nama': 'Bumi Serpong Damai', 'sektor': 'Properti', 'index': 'LQ45'},\n",
    "    'CTRA': {'nama': 'Ciputra Development', 'sektor': 'Properti', 'index': 'LQ45'},\n",
    "    'SMRA': {'nama': 'Summarecon Agung', 'sektor': 'Properti', 'index': 'LQ45'},\n",
    "    \n",
    "    # Cement & Construction\n",
    "    'SMGR': {'nama': 'Semen Indonesia', 'sektor': 'Konstruksi', 'index': 'LQ45'},\n",
    "    'INTP': {'nama': 'Indocement Tunggal Prakarsa', 'sektor': 'Konstruksi', 'index': 'LQ45'},\n",
    "    'WIKA': {'nama': 'Wijaya Karya', 'sektor': 'Konstruksi', 'index': 'LQ45'},\n",
    "    \n",
    "    # Tech & E-commerce\n",
    "    'GOTO': {'nama': 'GoTo Gojek Tokopedia', 'sektor': 'Teknologi', 'index': 'LQ45'},\n",
    "    'BUKA': {'nama': 'Bukalapak', 'sektor': 'Teknologi', 'index': 'IHSG'},\n",
    "    \n",
    "    # Healthcare\n",
    "    'KLBF': {'nama': 'Kalbe Farma', 'sektor': 'Kesehatan', 'index': 'LQ45,IDX30'},\n",
    "    'SIDO': {'nama': 'Industri Jamu Sido Muncul', 'sektor': 'Kesehatan', 'index': 'LQ45'},\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "stocks_df = pd.DataFrame([\n",
    "    {'kode': k, 'nama': v['nama'], 'sektor': v['sektor'], 'index': v['index']}\n",
    "    for k, v in STOCKS_DATA.items()\n",
    "])\n",
    "\n",
    "print(f\"ðŸ“Š Total {len(stocks_df)} saham tersedia\")\n",
    "stocks_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593eff80",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Web Scraper untuk Berita Saham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b05ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsScraper:\n",
    "    \"\"\"Scraper untuk mengambil berita saham dari berbagai sumber\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "    \n",
    "    def scrape_kontan(self, stock_code: str, max_articles: int = 5) -> List[Dict]:\n",
    "        \"\"\"Scrape berita dari Kontan.co.id\"\"\"\n",
    "        articles = []\n",
    "        search_url = f\"https://www.kontan.co.id/search/?search={stock_code}\"\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(search_url, timeout=15)\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "            \n",
    "            # Find article links\n",
    "            links = soup.find_all('a', href=True)\n",
    "            article_urls = []\n",
    "            \n",
    "            for link in links:\n",
    "                href = link.get('href', '')\n",
    "                if '/news/' in href or '/investasi/' in href:\n",
    "                    if href.startswith('http'):\n",
    "                        article_urls.append(href)\n",
    "                    elif href.startswith('/'):\n",
    "                        article_urls.append(f\"https://www.kontan.co.id{href}\")\n",
    "            \n",
    "            article_urls = list(set(article_urls))[:max_articles]\n",
    "            \n",
    "            for url in article_urls:\n",
    "                try:\n",
    "                    article = self._parse_kontan_article(url)\n",
    "                    if article:\n",
    "                        article['stock_code'] = stock_code\n",
    "                        articles.append(article)\n",
    "                    time.sleep(1)\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ Error scraping Kontan for {stock_code}: {e}\")\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    def _parse_kontan_article(self, url: str) -> Optional[Dict]:\n",
    "        \"\"\"Parse artikel Kontan\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=15)\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "            \n",
    "            # Title\n",
    "            title_tag = soup.find('h1') or soup.find('title')\n",
    "            title = title_tag.get_text(strip=True) if title_tag else ''\n",
    "            \n",
    "            # Content\n",
    "            content_div = soup.find('div', class_='tmpt-desk') or soup.find('article')\n",
    "            if content_div:\n",
    "                paragraphs = content_div.find_all('p')\n",
    "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
    "            else:\n",
    "                content = ''\n",
    "            \n",
    "            if title and len(content) > 100:\n",
    "                return {\n",
    "                    'title': title,\n",
    "                    'content': content[:2000],\n",
    "                    'url': url,\n",
    "                    'source': 'Kontan',\n",
    "                    'scraped_at': datetime.now()\n",
    "                }\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    def scrape_detik(self, stock_code: str, max_articles: int = 5) -> List[Dict]:\n",
    "        \"\"\"Scrape berita dari Detik Finance\"\"\"\n",
    "        articles = []\n",
    "        search_url = f\"https://www.detik.com/search/searchall?query={stock_code}&siteid=3\"\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(search_url, timeout=15)\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "            \n",
    "            # Find articles\n",
    "            article_items = soup.find_all('article')[:max_articles]\n",
    "            \n",
    "            for item in article_items:\n",
    "                try:\n",
    "                    link = item.find('a', href=True)\n",
    "                    if link:\n",
    "                        url = link.get('href')\n",
    "                        title = link.get_text(strip=True)\n",
    "                        \n",
    "                        if title and url:\n",
    "                            articles.append({\n",
    "                                'title': title,\n",
    "                                'content': title,  # Use title as content for search results\n",
    "                                'url': url,\n",
    "                                'source': 'Detik',\n",
    "                                'stock_code': stock_code,\n",
    "                                'scraped_at': datetime.now()\n",
    "                            })\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ Error scraping Detik for {stock_code}: {e}\")\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    def scrape_all(self, stock_code: str, max_per_source: int = 3) -> List[Dict]:\n",
    "        \"\"\"Scrape dari semua sumber\"\"\"\n",
    "        all_articles = []\n",
    "        \n",
    "        # Kontan\n",
    "        kontan_articles = self.scrape_kontan(stock_code, max_per_source)\n",
    "        all_articles.extend(kontan_articles)\n",
    "        \n",
    "        # Detik\n",
    "        detik_articles = self.scrape_detik(stock_code, max_per_source)\n",
    "        all_articles.extend(detik_articles)\n",
    "        \n",
    "        return all_articles\n",
    "\n",
    "\n",
    "# Initialize scraper\n",
    "scraper = NewsScraper()\n",
    "print(\"âœ… News Scraper ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3526bc8",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ NLP - Text Preprocessing & Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300eeddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndonesianNLP:\n",
    "    \"\"\"NLP Processing untuk Bahasa Indonesia\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Stemmer\n",
    "        factory = StemmerFactory()\n",
    "        self.stemmer = factory.createStemmer()\n",
    "        \n",
    "        # Stopwords\n",
    "        stop_factory = StopWordRemoverFactory()\n",
    "        self.stopword_remover = stop_factory.createStopWordRemover()\n",
    "        \n",
    "        # Sentiment Lexicon\n",
    "        self.positive_words = {\n",
    "            'naik', 'untung', 'profit', 'laba', 'positif', 'bagus', 'baik',\n",
    "            'meningkat', 'tumbuh', 'berkembang', 'sukses', 'berhasil', 'optimal',\n",
    "            'kuat', 'solid', 'stabil', 'prospek', 'potensi', 'peluang', 'rebound',\n",
    "            'bullish', 'outperform', 'buy', 'beli', 'akumulasi', 'dividen',\n",
    "            'ekspansi', 'inovasi', 'efisien', 'produktif', 'surplus', 'recovery',\n",
    "            'pulih', 'membaik', 'menguat', 'melesat', 'melonjak', 'meroket',\n",
    "            'tertinggi', 'rekor', 'fantastis', 'spektakuler', 'cemerlang',\n",
    "            'moncer', 'ciamik', 'oke', 'mantap', 'jos', 'top', 'keren'\n",
    "        }\n",
    "        \n",
    "        self.negative_words = {\n",
    "            'turun', 'rugi', 'loss', 'negatif', 'buruk', 'jelek', 'anjlok',\n",
    "            'menurun', 'merosot', 'jatuh', 'gagal', 'lemah', 'lesu', 'loyo',\n",
    "            'bearish', 'underperform', 'sell', 'jual', 'defisit', 'krisis',\n",
    "            'resesi', 'kontraksi', 'penurunan', 'kemerosotan', 'kerugian',\n",
    "            'koreksi', 'tertekan', 'melemah', 'terpuruk', 'kolaps', 'bangkrut',\n",
    "            'default', 'utang', 'beban', 'risiko', 'ancaman', 'masalah',\n",
    "            'kendala', 'hambatan', 'tantangan', 'sulit', 'berat', 'parah',\n",
    "            'terendah', 'minimum', 'minim', 'seret', 'stagnan', 'mandek'\n",
    "        }\n",
    "        \n",
    "        self.intensifiers = {'sangat', 'sekali', 'amat', 'paling', 'lebih', 'makin', 'semakin', 'begitu', 'terlalu'}\n",
    "        self.negations = {'tidak', 'bukan', 'belum', 'tanpa', 'jangan', 'tak', 'tiada'}\n",
    "    \n",
    "    def preprocess(self, text: str) -> str:\n",
    "        \"\"\"Preprocess text\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Remove stopwords\n",
    "        text = self.stopword_remover.remove(text)\n",
    "        \n",
    "        # Stemming\n",
    "        text = self.stemmer.stem(text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def analyze_sentiment(self, text: str) -> Dict:\n",
    "        \"\"\"Analyze sentiment of text\"\"\"\n",
    "        if not text:\n",
    "            return {'score': 0, 'label': 'netral', 'positive': 0, 'negative': 0}\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        words = text_lower.split()\n",
    "        \n",
    "        positive_count = 0\n",
    "        negative_count = 0\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "            # Check negation\n",
    "            is_negated = False\n",
    "            if i > 0 and words[i-1] in self.negations:\n",
    "                is_negated = True\n",
    "            \n",
    "            # Check intensifier\n",
    "            intensity = 1.0\n",
    "            if i > 0 and words[i-1] in self.intensifiers:\n",
    "                intensity = 1.5\n",
    "            \n",
    "            # Count sentiment\n",
    "            if word in self.positive_words:\n",
    "                if is_negated:\n",
    "                    negative_count += intensity\n",
    "                else:\n",
    "                    positive_count += intensity\n",
    "            elif word in self.negative_words:\n",
    "                if is_negated:\n",
    "                    positive_count += intensity\n",
    "                else:\n",
    "                    negative_count += intensity\n",
    "        \n",
    "        # Calculate score\n",
    "        total = positive_count + negative_count\n",
    "        if total == 0:\n",
    "            score = 0\n",
    "        else:\n",
    "            score = (positive_count - negative_count) / total\n",
    "        \n",
    "        # Determine label\n",
    "        if score > 0.1:\n",
    "            label = 'positif'\n",
    "        elif score < -0.1:\n",
    "            label = 'negatif'\n",
    "        else:\n",
    "            label = 'netral'\n",
    "        \n",
    "        return {\n",
    "            'score': round(score, 3),\n",
    "            'label': label,\n",
    "            'positive': positive_count,\n",
    "            'negative': negative_count\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize NLP\n",
    "nlp = IndonesianNLP()\n",
    "print(\"âœ… Indonesian NLP ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e15b8",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Content-Based Recommendation Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6736a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockRecommender:\n",
    "    \"\"\"Content-Based Stock Recommendation System\"\"\"\n",
    "    \n",
    "    def __init__(self, nlp_processor: IndonesianNLP):\n",
    "        self.nlp = nlp_processor\n",
    "        self.vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "        self.stock_profiles = {}\n",
    "        self.news_data = {}\n",
    "    \n",
    "    def add_news(self, stock_code: str, articles: List[Dict]):\n",
    "        \"\"\"Add news articles for a stock\"\"\"\n",
    "        if stock_code not in self.news_data:\n",
    "            self.news_data[stock_code] = []\n",
    "        \n",
    "        for article in articles:\n",
    "            # Analyze sentiment\n",
    "            full_text = f\"{article.get('title', '')} {article.get('content', '')}\"\n",
    "            sentiment = self.nlp.analyze_sentiment(full_text)\n",
    "            \n",
    "            article['sentiment'] = sentiment\n",
    "            article['processed_text'] = self.nlp.preprocess(full_text)\n",
    "            \n",
    "            self.news_data[stock_code].append(article)\n",
    "    \n",
    "    def build_stock_profile(self, stock_code: str) -> Dict:\n",
    "        \"\"\"Build profile for a stock based on news\"\"\"\n",
    "        if stock_code not in self.news_data or not self.news_data[stock_code]:\n",
    "            return None\n",
    "        \n",
    "        articles = self.news_data[stock_code]\n",
    "        \n",
    "        # Aggregate sentiment\n",
    "        sentiments = [a['sentiment']['score'] for a in articles]\n",
    "        avg_sentiment = np.mean(sentiments) if sentiments else 0\n",
    "        \n",
    "        # Count labels\n",
    "        labels = [a['sentiment']['label'] for a in articles]\n",
    "        positive_count = labels.count('positif')\n",
    "        negative_count = labels.count('negatif')\n",
    "        neutral_count = labels.count('netral')\n",
    "        \n",
    "        # Combine all text\n",
    "        combined_text = ' '.join([a['processed_text'] for a in articles if a['processed_text']])\n",
    "        \n",
    "        # Calculate recommendation score (0-100)\n",
    "        sentiment_score = (avg_sentiment + 1) / 2 * 50  # 0-50\n",
    "        volume_score = min(len(articles) * 5, 30)  # 0-30\n",
    "        positive_ratio = positive_count / len(articles) * 20 if articles else 0  # 0-20\n",
    "        \n",
    "        recommendation_score = sentiment_score + volume_score + positive_ratio\n",
    "        \n",
    "        # Determine label\n",
    "        if recommendation_score >= 70:\n",
    "            rec_label = 'Strong Buy'\n",
    "        elif recommendation_score >= 55:\n",
    "            rec_label = 'Buy'\n",
    "        elif recommendation_score >= 45:\n",
    "            rec_label = 'Hold'\n",
    "        elif recommendation_score >= 30:\n",
    "            rec_label = 'Sell'\n",
    "        else:\n",
    "            rec_label = 'Strong Sell'\n",
    "        \n",
    "        profile = {\n",
    "            'stock_code': stock_code,\n",
    "            'avg_sentiment': round(avg_sentiment, 3),\n",
    "            'positive_count': positive_count,\n",
    "            'negative_count': negative_count,\n",
    "            'neutral_count': neutral_count,\n",
    "            'total_news': len(articles),\n",
    "            'recommendation_score': round(recommendation_score, 2),\n",
    "            'recommendation_label': rec_label,\n",
    "            'combined_text': combined_text\n",
    "        }\n",
    "        \n",
    "        self.stock_profiles[stock_code] = profile\n",
    "        return profile\n",
    "    \n",
    "    def find_similar_stocks(self, stock_code: str, top_n: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Find similar stocks based on news content\"\"\"\n",
    "        if stock_code not in self.stock_profiles:\n",
    "            return []\n",
    "        \n",
    "        # Get all profiles with text\n",
    "        profiles = [(k, v['combined_text']) for k, v in self.stock_profiles.items() if v['combined_text']]\n",
    "        \n",
    "        if len(profiles) < 2:\n",
    "            return []\n",
    "        \n",
    "        codes, texts = zip(*profiles)\n",
    "        \n",
    "        # TF-IDF Vectorization\n",
    "        try:\n",
    "            tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "        except:\n",
    "            return []\n",
    "        \n",
    "        # Find index of target stock\n",
    "        try:\n",
    "            target_idx = codes.index(stock_code)\n",
    "        except ValueError:\n",
    "            return []\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarities = cosine_similarity(tfidf_matrix[target_idx:target_idx+1], tfidf_matrix)[0]\n",
    "        \n",
    "        # Get top similar (excluding self)\n",
    "        similar_indices = similarities.argsort()[::-1][1:top_n+1]\n",
    "        \n",
    "        results = [(codes[i], round(similarities[i], 3)) for i in similar_indices if similarities[i] > 0]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_recommendations(self) -> pd.DataFrame:\n",
    "        \"\"\"Get all stock recommendations sorted by score\"\"\"\n",
    "        if not self.stock_profiles:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        data = []\n",
    "        for code, profile in self.stock_profiles.items():\n",
    "            stock_info = STOCKS_DATA.get(code, {})\n",
    "            data.append({\n",
    "                'Kode': code,\n",
    "                'Nama': stock_info.get('nama', ''),\n",
    "                'Sektor': stock_info.get('sektor', ''),\n",
    "                'Skor': profile['recommendation_score'],\n",
    "                'Rekomendasi': profile['recommendation_label'],\n",
    "                'Avg Sentiment': profile['avg_sentiment'],\n",
    "                'Berita (+)': profile['positive_count'],\n",
    "                'Berita (-)': profile['negative_count'],\n",
    "                'Total Berita': profile['total_news']\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df = df.sort_values('Skor', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "# Initialize recommender\n",
    "recommender = StockRecommender(nlp)\n",
    "print(\"âœ… Stock Recommender ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315deb2b",
   "metadata": {},
   "source": [
    "---\n",
    "## 7ï¸âƒ£ Jalankan Analisis\n",
    "\n",
    "Pilih saham yang ingin dianalisis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf5d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pilih saham untuk dianalisis\n",
    "# Ubah list ini sesuai kebutuhan\n",
    "\n",
    "SELECTED_STOCKS = ['BBCA', 'BBRI', 'TLKM', 'ASII', 'ADRO', 'GOTO', 'ANTM', 'KLBF']\n",
    "\n",
    "print(f\"ðŸŽ¯ Saham yang akan dianalisis: {SELECTED_STOCKS}\")\n",
    "print(f\"ðŸ“° Memulai scraping berita...\\n\")\n",
    "\n",
    "# Scrape news for each stock\n",
    "for stock in SELECTED_STOCKS:\n",
    "    print(f\"ðŸ“¥ Scraping berita untuk {stock}...\")\n",
    "    articles = scraper.scrape_all(stock, max_per_source=3)\n",
    "    \n",
    "    if articles:\n",
    "        recommender.add_news(stock, articles)\n",
    "        recommender.build_stock_profile(stock)\n",
    "        print(f\"   âœ… {len(articles)} artikel ditemukan\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ Tidak ada artikel ditemukan\")\n",
    "    \n",
    "    time.sleep(2)  # Delay untuk menghindari rate limiting\n",
    "\n",
    "print(\"\\nâœ… Scraping selesai!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36545ddb",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Hasil Rekomendasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd1aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampilkan hasil rekomendasi\n",
    "recommendations = recommender.get_recommendations()\n",
    "\n",
    "if not recommendations.empty:\n",
    "    print(\"ðŸ“Š HASIL REKOMENDASI SAHAM\")\n",
    "    print(\"=\" * 80)\n",
    "    display(recommendations)\n",
    "else:\n",
    "    print(\"âš ï¸ Belum ada data untuk ditampilkan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057736ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi Rekomendasi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not recommendations.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar chart - Recommendation Score\n",
    "    colors = ['#2ecc71' if x >= 55 else '#f39c12' if x >= 45 else '#e74c3c' \n",
    "              for x in recommendations['Skor']]\n",
    "    axes[0].barh(recommendations['Kode'], recommendations['Skor'], color=colors)\n",
    "    axes[0].set_xlabel('Skor Rekomendasi')\n",
    "    axes[0].set_title('ðŸ“ˆ Skor Rekomendasi Saham')\n",
    "    axes[0].axvline(x=55, color='green', linestyle='--', alpha=0.5, label='Buy Threshold')\n",
    "    axes[0].axvline(x=45, color='red', linestyle='--', alpha=0.5, label='Sell Threshold')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Bar chart - Sentiment\n",
    "    colors_sent = ['#2ecc71' if x > 0 else '#e74c3c' if x < 0 else '#95a5a6' \n",
    "                   for x in recommendations['Avg Sentiment']]\n",
    "    axes[1].barh(recommendations['Kode'], recommendations['Avg Sentiment'], color=colors_sent)\n",
    "    axes[1].set_xlabel('Rata-rata Sentimen')\n",
    "    axes[1].set_title('ðŸ’¬ Sentimen Berita')\n",
    "    axes[1].axvline(x=0, color='gray', linestyle='-', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ddff5",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ Analisis Detail per Saham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee73827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pilih saham untuk analisis detail\n",
    "DETAIL_STOCK = 'BBCA'  # Ubah sesuai kebutuhan\n",
    "\n",
    "if DETAIL_STOCK in recommender.stock_profiles:\n",
    "    profile = recommender.stock_profiles[DETAIL_STOCK]\n",
    "    stock_info = STOCKS_DATA.get(DETAIL_STOCK, {})\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸ“Š ANALISIS DETAIL: {DETAIL_STOCK}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Nama       : {stock_info.get('nama', '-')}\")\n",
    "    print(f\"Sektor     : {stock_info.get('sektor', '-')}\")\n",
    "    print(f\"Index      : {stock_info.get('index', '-')}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nðŸ“ˆ HASIL ANALISIS:\")\n",
    "    print(f\"   Skor Rekomendasi : {profile['recommendation_score']}\")\n",
    "    print(f\"   Label            : {profile['recommendation_label']}\")\n",
    "    print(f\"   Avg Sentiment    : {profile['avg_sentiment']}\")\n",
    "    print(f\"\\nðŸ“° STATISTIK BERITA:\")\n",
    "    print(f\"   Total Berita  : {profile['total_news']}\")\n",
    "    print(f\"   Positif       : {profile['positive_count']}\")\n",
    "    print(f\"   Negatif       : {profile['negative_count']}\")\n",
    "    print(f\"   Netral        : {profile['neutral_count']}\")\n",
    "    \n",
    "    # Show similar stocks\n",
    "    similar = recommender.find_similar_stocks(DETAIL_STOCK, top_n=3)\n",
    "    if similar:\n",
    "        print(f\"\\nðŸ”— SAHAM SERUPA:\")\n",
    "        for code, score in similar:\n",
    "            print(f\"   - {code}: {score:.1%} similarity\")\n",
    "    \n",
    "    # Show news headlines\n",
    "    if DETAIL_STOCK in recommender.news_data:\n",
    "        print(f\"\\nðŸ“° HEADLINE BERITA:\")\n",
    "        for i, article in enumerate(recommender.news_data[DETAIL_STOCK][:5], 1):\n",
    "            sentiment = article['sentiment']\n",
    "            emoji = 'ðŸŸ¢' if sentiment['label'] == 'positif' else 'ðŸ”´' if sentiment['label'] == 'negatif' else 'âšª'\n",
    "            print(f\"   {i}. {emoji} {article['title'][:70]}...\")\n",
    "            print(f\"      Sumber: {article['source']} | Sentimen: {sentiment['score']}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Data untuk {DETAIL_STOCK} tidak tersedia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1285d7c2",
   "metadata": {},
   "source": [
    "## ðŸ”Ÿ Cari Saham Serupa (Content-Based Filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d2a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cari saham serupa berdasarkan konten berita\n",
    "TARGET_STOCK = 'BBCA'  # Ubah sesuai kebutuhan\n",
    "\n",
    "similar_stocks = recommender.find_similar_stocks(TARGET_STOCK, top_n=5)\n",
    "\n",
    "if similar_stocks:\n",
    "    print(f\"\\nðŸ” Saham yang mirip dengan {TARGET_STOCK}:\")\n",
    "    print(\"-\" * 40)\n",
    "    for code, similarity in similar_stocks:\n",
    "        stock_info = STOCKS_DATA.get(code, {})\n",
    "        profile = recommender.stock_profiles.get(code, {})\n",
    "        print(f\"  {code}: {stock_info.get('nama', '')}\")\n",
    "        print(f\"     Similarity: {similarity:.1%}\")\n",
    "        print(f\"     Skor: {profile.get('recommendation_score', 0)} ({profile.get('recommendation_label', '-')})\")\n",
    "        print()\n",
    "else:\n",
    "    print(f\"âš ï¸ Tidak cukup data untuk mencari saham serupa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3933db",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“ Catatan\n",
    "\n",
    "- **Skor Rekomendasi** dihitung berdasarkan:\n",
    "  - Rata-rata sentimen berita\n",
    "  - Volume berita yang tersedia\n",
    "  - Rasio berita positif\n",
    "\n",
    "- **Kategori Rekomendasi:**\n",
    "  - ðŸŸ¢ Strong Buy: Skor â‰¥ 70\n",
    "  - ðŸŸ¢ Buy: Skor â‰¥ 55\n",
    "  - ðŸŸ¡ Hold: Skor â‰¥ 45\n",
    "  - ðŸ”´ Sell: Skor â‰¥ 30\n",
    "  - ðŸ”´ Strong Sell: Skor < 30\n",
    "\n",
    "- **Disclaimer:** Sistem ini hanya untuk edukasi dan referensi. Bukan rekomendasi investasi finansial.\n",
    "\n",
    "---\n",
    "Â© 2025 Stock Recommendation System"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
